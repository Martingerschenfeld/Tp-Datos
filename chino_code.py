# -*- coding: utf-8 -*-
"""Chino_Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uZfD0SyaQiBpMlWrtTNCIgQrNMA1D4HW
"""



!pip install tweepy

"""Paquetes necesarios para ejecutar el archivo"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re as re
import tweepy as tw
import itertools
from collections import OrderedDict

from google.colab import drive
drive.mount('/content/drive')

df_twitter = pd.read_csv('/content/drive/My Drive/Org_datos/train.csv', sep=',', header=0)
df_twitter['target']
display(df_twitter)

"""Análisis Exploratorio de Datos"""

# Information about the dataset
df_twitter.info()

"""#Veo la dimension del dataframe"""

#veo la dimension del dataframe
print (df_twitter.shape)

"""#Cuento las cantidades de datos no nulos que tiene por columna"""

#cuento las cantidades de datos no nulos que tiene por columna
df_twitter.count ()

"""## Palabrás Más Utilizadas"""

tweets = df_twitter.text.tolist()
tweets_token = []
for i in range(0,df_twitter.text.count()):
  tweet_split = tweets[i].split()
  tweets_token.append(tweet_split)  

tweets_token_list  = [token for sublist in tweets_token for token in sublist]
tweets_token_list = [x.lower() for x in tweets_token_list]


unique_words = {}
for tweet in tweets_token_list:
    if (tweet in unique_words.keys()):
          unique_words[tweet] += 1
    else:
          unique_words[tweet] = 1

sorted_tweet = OrderedDict(sorted(unique_words.items(), 
                                  key=lambda kv: kv[1], reverse=True))

list(sorted_tweet.items())[:15]

#Nos quedamos con los keys cuyo longitud es mayor a 4
unique_words_long4 ={}
for key, value in sorted_tweet.items():
  if len(key)>4:
    unique_words_long4[key] = value

#Primeros 15 resultados
top15 = dict(list(unique_words_long4.items())[0: 15])
plt.bar(top15.keys(), top15.values(), color='g')
plt.title("Histograma de las Palabras más frecuentes ", fontsize=16)
plt.xticks(rotation='vertical')
plt.xlabel('Palabras más frecuentes que mayor a 4 letras')
plt.ylabel('Frecuencia')
plt.show()

#analizo las 3 variables, que contienen

location = df_twitter['location']

"""## Análisis por Ubicación"""

type (location)

location.value_counts()

country= {'New York':'United States','USA':'United States','Los Angeles, CA':'United States', 'Washington, DC':'United States','Chicago, IL':'United States','California':'United States','California, USA':'United States','New York, NY':'United States','San Francisco':'United States', 'Florida':'United States', 'Washington, D.C.':'United States','Los Angeles':'United States','NYC':'United States','Seattle':'United States','San Francisco, CA':'United States','Chicago':'United States', 'Atlanta, GA':'United States', 'Sacramento, CA':'United States', 'Chicago Area':'United States', 'Los Angles, CA':'United States','Palm Beach County, FL':'United States','Texas':'United States','CA, AZ & NV':'United States', 'Colonial Heights, VA':'United States','New York City':'United States', 'Nashville, TN':'United States', 'US':'United States','San Diego,CA':'United States','Dallas, TX':'United States','Denver, Colorado':'United States','San Diego, CA':'United States','East TN':'United States', 'Houston, TX':'United States', 'Seattle, WA':'United States', 'Chapel Hill, NC':'United States','Austin,TX':'United States','Pennsylvania, USA':'United States', 'Denver, CO':'United States', 'Tennessee':'United States','Austin, TX':'United States','Memphis, TN':'United States','Orlando, FL':'United States','Paterson, New Jersey':'United States','Colorado':'United States','Massachusetts':'United States','Oklahoma City, OK':'United States','Charlotte, NC':'United States','Salt Lake City, Utah':'United States','Paterson, New Jersey':'United States','Portland, OR':'United States','Paterson, New Jersey ':'United Statess','Iliff,Colorado ':'United States','Brookly, NY': 'United States', 'United Statess':'United States','Atlanta':'United States', 'Brooklyn, NY':'United States', 'California, United States':'United States','NY':'United States','Wisconsin':'United States', 'San Jose, CA':'United States','New York, USA':'United States','Souther California':'United States','New Hampshire':'United States', 'Southern California':'United States','Florida, USA':'United States','Pennsylvania':'United States','Cleveland, OH':'United States','Indiana':'United States','Puerto Rico':'United States','Asheville,NC':'United States','Boston, MA':'United States','New Jersey':'United States','North Carolina':'United States','Tampa, FL':'United States','Texas, USA':'United States', 'North Carolina, USA':'United States', 'Midwest':'United States','Asheville, NC':'United States','Maryland':'United States', 'Atlanta Georgia ':'United States', 'San Francisco Bay Area':'United States','Oakland, CA':'United States','Oregon':'United States','Republic of Texas':'United States','Sacramento':'United States','Phoenix, AZ':'United States','Las Vegas, Nevada':'United States','Massachusetts, USA':'United States','U.S.A':'United States','Kansas City':'United States','Michigan':'United States','Portland, Oregon':'United States','Seattle, Washington':'United States','Philadelphia, PA':'United States','Bend, Oregon':'United States','Haddonfield, NJ':'United States','United Statess':'United States',
          'London':'United Kingdom','UK':'United Kingdom', 'Ireland':'United Kingdom','Londo, UK':'United Kingdom', 'London, England':'United Kingdom', 'Newport, Wales, UK':'United Kingdom','London, UK':'United Kingdom','Manchester':'United Kingdom', 'Scotland':'United Kingdom', 'Newcastle':'United Kingdom','Coventry':'United Kingdom','Leeds, England':'United Kingdom', 'Edinburgh':'United Kingdom','Paignton':'United Kingdom','Leicester':'United Kingdom','Birmingham':'United Kingdom','Manchester, England':'United Kingdom','london':'United Kingdom','uk':'United Kingdom',
          'Mumbai':'India', "Jakarta/Kuala Lumpur/S'pore":'India','Mumbai, Maharashtra':'India',
          'Toronto':'Canada','Canada ':'Canada','Calgary, Alberta':'Canada','Calgary':'Canada', 'Calgary, AB':'Canada','Vancouver, BC':'Canada','British Columbia, Canada':'Canada', 'canada':'Canada',
          'china':'China',
          'Santo Domingo Alma Rosa':'Dominica Republic',
          'Sydney':'Australia','Melbourne, Australia':'Australia','Melbourne':'Australia', 'Sydney, New South Wales':'Australia','Sydney, Australia':'Australia',
          'Morioh, Japan':'Japan','Tokyo':'Japan',
          'Brasil':'Brazil',
          'Cape Town':'South Africa',
          'Lagos':'Nigeria','Lagos, Nigeria':'Nigeria','Port Harcourt, Nigeria':'Nigeria',
          'Geneva':'Switzerland',
          'Nairobi':'Kenya','Nairobi-KENYA':'Kenya',
          'Worldwide': 'NaN','worldwide':'NaN','WorldWide':'NaN','Everywhere': 'NaN','Earth':'NaN', 'ss':'NaN','WonderlandÛÓ ?????? ???? ??????':'NaN','304':'NaN','World':'NaN', 'The Globe':'NaN', '#BlackLivesMatter':'NaN','IDN':'NaN', '??????':'NaN', ' Road to the Billionaires Club':'NaN','Global':'NaN','Planet Earth':'NaN','Pedophile hunting ground':'NaN','in the Word of God':'NaN', '?':'NaN','Happily Married with 2 kids ':'NaN','??':'NaN', '?????':'NaN','MAD as Hell':'NaN'     }
location = location.map(country, na_action='ignore').fillna('NaN')
print(location.value_counts())



"""## Análisis por Longitud del Tweet"""

#dropeo location, y me fijo la longitud de caracteres de cada tweet

DF_drop_leng = df_twitter.drop(columns=['location'])
DF_drop_leng['leng_tweet'] = df_twitter['text'].str.len()
DF_drop_leng.head(20)

#######aca hice el histograma de longitud de los tweets, es distinto al de densidad


g = DF_drop_leng["leng_tweet"].plot.hist(bins=30, color='skyblue')
g.set_title("Histograma de longitud de los tweets", fontsize=16)
g.set_xlabel("Longitud",fontsize=18)
g.set_ylabel("Frecuencia", fontsize=18)

# analizo algunas variables estadisticas

gr_by_target = DF_drop_leng.groupby(['target']).agg({'leng_tweet': ['mean', 'count', 'max' , 'min']})
gr_by_target.head()

g = sns.boxplot(x="target", y="leng_tweet", data=DF_drop_leng, palette="Paired")
g.set_title("Longitud tweets Según Veracidad", fontsize=15)
g.set_xlabel("target real", fontsize=10)
g.set_ylabel("Length", fontsize=10)

# analizo algunas variables estadisticas

gr_by_target = key_top_39.groupby(['target']).agg({'leng_tweet': ['mean', 'count', 'max' , 'min']})
gr_by_target.head()

"""## Análisis por Keyword"""

key = df_twitter['keyword']
print(key.value_counts().sum()) # Cantidad de Tweets con keyword
print(key.value_counts()) # Cantidad de Categorías

key_counts = DF_drop_leng['keyword'].value_counts()
key_counts[key_counts > key_counts.mean()]

"""Me quedo con los keywords con mas de 39 ocurrencias"""

key_top_39 = DF_drop_leng.groupby("keyword").filter(lambda x: len(x) >= 39)

print(key_top_39)

key_top_39['keyword'].value_counts().sort_index().plot(kind='bar', figsize=(15,5))
plt.title('Frecuencia de Keywords con mayor a 39 entradas')

# analizo algunas variables estadisticas

gr_by_keyword1 = (key_top_39 [key_top_39['target'] == 0]).groupby(['keyword']).agg({'target': ['mean', 'count', 'max' , 'min']})
gr_by_keyword1= gr_by_keyword1.target #Eliminando el multi-index

gr_by_keyword2 = (key_top_39[key_top_39['target'] == 1]).groupby(['keyword']).agg({'target': [ 'count', 'max' , 'min']})
gr_by_keyword2= gr_by_keyword2.target #Eliminando el multi-index
gr_by_keyword2.head(10)

type(gr_by_keyword1['count'])
rate_of_truthfulnes = pd.concat([gr_by_keyword1['count'], gr_by_keyword2['count']], axis = 1, keys = ['fake','truth'])
rate_of_truthfulnes['total'] = rate_of_truthfulnes.fake+rate_of_truthfulnes.truth
rate_of_truthfulnes['%'] = rate_of_truthfulnes.truth/rate_of_truthfulnes.total

print(rate_of_truthfulnes)

labels = key_top_39['keyword'].drop_duplicates()
fake = rate_of_truthfulnes.fake
truth = rate_of_truthfulnes.truth

fig, ax = plt.subplots()


ax.bar(labels, fake, label='Fake')
ax.bar(labels, truth, label='Truth',bottom=fake)
plt.xticks(rotation='vertical')
xlocs, xlabs = plt.xticks()

ax.set_ylabel('Veracidad')
ax.set_xlabel('Keyword')
ax.set_title('Falsedad o Veracidad del Tweet')
ax.legend()

"""#Más mencionados según veracidad"""

df_twitter['target'].fillna(0)
sText = df_twitter['text']
sTarget = df_twitter['target']

#Busco todas las menciones de los usuarios
exp = '@[_a-zA-Z]{2,16}'
mention_on_real_tw = {}
j=0;
for i in range(len(sText)):
    if(len(re.findall(exp,sText[i]))>1 and sTarget[i]==1):
        mention_on_real_tw[j] = re.findall(exp,sText[i])
        j+=1

arrobed_real = {}
k= 0
for i in range(len(mention_on_real_tw)):
    for j in range(len(mention_on_real_tw[i])):
        arrobed_real[k] = mention_on_real_tw[i][j]
        k+=1

        
        
mention_on_fake_tw  ={}
j=0;
for i in range(len(sText)):
    if(len(re.findall(exp,sText[i]))>1 and sTarget[i]==0):
        mention_on_fake_tw[j] = re.findall(exp,sText[i])
        j+=1
        
arrobed_fake = {}
k= 0
for i in range(len(mention_on_fake_tw)):
    for j in range(len(mention_on_fake_tw[i])):
        arrobed_fake[k] = mention_on_fake_tw[i][j]
        k+=1
        
        
pdReal = pd.DataFrame.from_dict(arrobed_real, orient='index', columns=['mentionReal',])
pdFake = pd.DataFrame.from_dict(arrobed_fake, orient='index', columns=['mentionFake',])

ax = pdReal['mentionReal'].value_counts().iloc[:9].plot(kind='bar')
ax.set_ylabel('Menciones')
ax.set_xlabel('Usuarios mencionados')
ax.set_title('Usuarios Mencionados Ante Tweets Veraces')

ax = pdFake['mentionFake'].value_counts().iloc[:9].plot(kind='bar')
ax.set_title('Usuarios Mencionados Ante Tweets Falsos')
ax.set_ylabel('Menciones')
ax.set_xlabel('Usuarios mencionados')

"""Análisis Heatmap"""

my_base = pd.concat([DF_drop_leng['leng_tweet'], sTarget], axis=1, join='inner')
correlation_matrix = my_base.corr(method='spearman')
correlation_matrix
figure_size = (2, 2)
fig, ax = plt.subplots(figsize=figure_size)

sns.heatmap(correlation_matrix, xticklabels=list(correlation_matrix), yticklabels=list(correlation_matrix),
            annot=True, fmt='.1f', linewidths = 0.5, ax=ax).set_title("Heatmap - Longitud del Tweet y Veracidad")